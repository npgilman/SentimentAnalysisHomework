{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "754e7235",
   "metadata": {},
   "source": [
    "<h1>Step 1: Exploratory Data Analysis.</h1></br> This stage is the very initial stage of your\n",
    "data analysis. You may want to know the size and sentiment distribution of\n",
    "the dataset. You may also want to examine if there are any missing values.\n",
    "This initial data analysis stage helps you to have a better understanding of the\n",
    "dataset before you build your sentiment classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253577ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('data/train.csv').sample(frac=1, axis=1).sample(frac=1).reset_index(drop=True)\n",
    "test_data = pd.read_csv('data/test.csv').sample(frac=1, axis=1).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)\n",
    "#train_data.isnull().sum()\n",
    "#train_data['Sentiment'].value_counts()\n",
    "#train_data['Sentiment'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8252",
   "metadata": {},
   "source": [
    "<h1>Step 2: Text Preprocessing.</h1></br> You need to prepare your training and testing\n",
    "dataset. Specifically for this problem, you need to preprocess the discussion\n",
    "texts, you may want to convert all words into lowercase and remove digital\n",
    "numbers and special characters. Please refer to our slides and class\n",
    "discussions for a full list of text preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d52640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra whitespace and convert upper to lower\n",
    "train_data = train_data.convert_dtypes()\n",
    "train_data['Text'] = train_data['Text'].str.strip()\n",
    "train_data['Text'] = train_data['Text'].str.lower()\n",
    "\n",
    "test_data = test_data.convert_dtypes()\n",
    "test_data['Text'] = test_data['Text'].str.strip()\n",
    "test_data['Text'] = test_data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0c3f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "import re\n",
    "# regex removes urls, user mentions and special characters respectively\n",
    "regex_pattern = re.compile('(http)[\\S]* ?|(@)[\\S]* ?|[^A-Z^a-z ]', flags=re.IGNORECASE)\n",
    "train_data['Text'] = train_data['Text'].str.replace(regex_pattern, '', regex=True)\n",
    "train_data['Text'] = train_data['Text'].str.split()\n",
    "\n",
    "test_data['Text'] = test_data['Text'].str.replace(regex_pattern, '', regex=True)\n",
    "test_data['Text'] = test_data['Text'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and stemming\n",
    "# (based on research, stemming seems like a better choice for sentiment analysis rather than lemmatization)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# removing and stemming in one pass takes about 3 minutes\n",
    "train_data['Text'] = train_data['Text'].apply(lambda text : [stemmer.stem(word) for word in text if not word in english_stopwords])\n",
    "test_data['Text'] = test_data['Text'].apply(lambda text : [stemmer.stem(word) for word in text if not word in english_stopwords])\n",
    "\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca360c2",
   "metadata": {},
   "source": [
    "<h1>Step 3: Linguistic Feature Extraction.</h1></br> You will extract linguistic features\n",
    "from the processed texts. You may consider a wide range of features we\n",
    "covered in the class, including bag-of-words, tf*idf, word2vec, etc. You may\n",
    "also consider other word-embedding semantic features such as Glove or\n",
    "BERT, but these are not required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60370cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the 5,000 most frequent words from a set of roughly 250,000 unique words\n",
    "import itertools\n",
    "\n",
    "# create a dictionary of all words with counts of 0\n",
    "corpus = dict.fromkeys(set(itertools.chain.from_iterable(train_data.Text)), 0)\n",
    "for word in itertools.chain.from_iterable(train_data.Text):\n",
    "    corpus[word] = corpus[word] + 1\n",
    "    \n",
    "d = dict(sorted(corpus.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "new_dict = {}\n",
    "index = 0\n",
    "for key, val in d.items():\n",
    "    if (index >= 1000):\n",
    "        break\n",
    "    index = index + 1\n",
    "    new_dict[key] = d[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words Feature Extraction: Takes roughly 30 seconds\n",
    "\n",
    "# initialize array to hold counts for each sentence\n",
    "arr = []\n",
    "keys = new_dict.keys()\n",
    "# loop through 10% of the data due to time constraints\n",
    "# create frequency counts for each key word in each sentence\n",
    "for i in range(50000):\n",
    "    test = dict.fromkeys(keys, 0)\n",
    "    for word in train_data.at[i, 'Text']:\n",
    "        if (word in keys):\n",
    "            test[word] = test[word] + 1\n",
    "    arr.append(test)\n",
    "\n",
    "# initialize bag of words dataframe\n",
    "bow_df = pd.DataFrame.from_dict(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb34700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF*IDF Feature Extraction: Takes roughly 1 minute 30 seconds\n",
    "import math\n",
    "\n",
    "# Term frequency:\n",
    "# N (word)/N (total words in this sentence)\n",
    "tf_arr = []\n",
    "\n",
    "# loop through 5% of the data due to time constraints\n",
    "# create term frequency dicts for each sentence and store them all in one array\n",
    "for i in range(50000):\n",
    "    word_list = train_data.at[i, 'Text']\n",
    "    tf_dict = {}\n",
    "    wordCount = len(word_list)\n",
    "    if wordCount == 0:\n",
    "        continue\n",
    "    word_freq = dict.fromkeys(keys, 0)\n",
    "    for word in word_list:\n",
    "        if word_freq.get(word, -1) != -1:\n",
    "            word_freq[word] += 1\n",
    "    for word, count in word_freq.items():\n",
    "            tf_dict[word] = count / wordCount\n",
    "    tf_arr.append(tf_dict)\n",
    "\n",
    "# Inverse document frequency:\n",
    "# log10 (N (sentences in the dataset)/N (sentences that contains the word)\n",
    "# initalize variables for later use\n",
    "n = len(train_data.index)\n",
    "idf_dict = dict.fromkeys(keys, 0)\n",
    "\n",
    "# determine the number of sentences that have a certain word in it\n",
    "for arr_dict in arr:\n",
    "    for word, count in arr_dict.items():\n",
    "        if int(count) > 0:\n",
    "            idf_dict[word] += 1\n",
    "# using the previously determined value,\n",
    "# divide the total number of sentences by the number of sentences containing the certain word        \n",
    "for word, count in idf_dict.items():\n",
    "    if count == 0:\n",
    "        idf_dict[word] = math.log(n)\n",
    "    else:\n",
    "        idf_dict[word] = math.log(n/float(count))\n",
    "\n",
    "# having just found the idf, multiply that against the tf for each sentence\n",
    "tf_idf_arr = []\n",
    "index = 0\n",
    "for arr_dict in arr:\n",
    "    tf_idf_dict = {}\n",
    "    for word, count in arr_dict.items():\n",
    "        tf_idf_dict[word] = count * idf_dict[word]\n",
    "    tf_idf_arr.append(tf_idf_dict)\n",
    "    \n",
    "# initialize tf_idf dataframe\n",
    "tf_idf_df = pd.DataFrame.from_dict(tf_idf_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vect Feature Extraction: Takes roughly 30 seconds\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=train_data.Text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "wv = model.wv\n",
    "\n",
    "wv_arr = []\n",
    "for i in range(50000):\n",
    "    wv_arr.append(wv[i])\n",
    "\n",
    "wv_df = pd.DataFrame(wv_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029cc45d",
   "metadata": {},
   "source": [
    "<h1>Step 4: Build your sentiment classification model.</h1></br> Provide the extracted\n",
    "set of linguistic features from the training dataset to your classification model.\n",
    "Note that this is a binary classification problem. You may want to start with\n",
    "classical machine learning algorithms such as Logistic Regression, SVM, Naive\n",
    "Bayes, and Random Forest. You may also consider neural-network-based\n",
    "classifiers, such as multilayer perceptron, but these are not required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import popular libraries to utilize later\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2684b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bow_df = train_data.merge(bow_df, how=\"right\", left_index=True, right_index=True).drop('Index', axis=1).drop('Text', axis=1)\n",
    "new_tfidf_df = train_data.merge(tf_idf_df, how=\"right\", left_index=True, right_index=True).drop('Index', axis=1).drop('Text', axis=1)\n",
    "new_wv_df = train_data.merge(wv_df, how=\"right\", left_index=True, right_index=True).drop('Index', axis=1).drop('Text', axis=1)\n",
    "\n",
    "# dependent and independent values\n",
    "y_bow = new_bow_df['Sentiment'].to_numpy()\n",
    "X_bow = new_bow_df.drop('Sentiment',axis=1).to_numpy()\n",
    "y_tfidf = new_tfidf_df['Sentiment'].to_numpy()\n",
    "X_tfidf = new_tfidf_df.drop('Sentiment',axis=1).to_numpy()\n",
    "y_wv = new_wv_df['Sentiment'].to_numpy()\n",
    "X_wv = new_wv_df.drop('Sentiment',axis=1).to_numpy()\n",
    "\n",
    "scale = StandardScaler()\n",
    "scaled_X_bow = scale.fit_transform(X_bow)\n",
    "scaled_X_tfidf = scale.fit_transform(X_tfidf)\n",
    "scaled_X_wv = scale.fit_transform(X_wv)\n",
    "\n",
    "X_bow_train, X_bow_test, y_bow_train, y_bow_test = train_test_split(scaled_X_bow, y_bow, test_size = 0.2)\n",
    "y_bow_train = y_bow_train.astype('int')\n",
    "X_bow_train = X_bow_train.astype('int')\n",
    "y_bow_test = y_bow_test.astype('int')\n",
    "X_bow_test = X_bow_test.astype('int')\n",
    "\n",
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(scaled_X_tfidf, y_tfidf, test_size = 0.2)\n",
    "y_tfidf_train = y_tfidf_train.astype('int')\n",
    "X_tfidf_train = X_tfidf_train.astype('int')\n",
    "y_tfidf_test = y_tfidf_test.astype('int')\n",
    "X_tfidf_test = X_tfidf_test.astype('int')\n",
    "\n",
    "X_wv_train, X_wv_test, y_wv_train, y_wv_test = train_test_split(scaled_X_wv, y_wv, test_size = 0.2)\n",
    "y_wv_train = y_wv_train.astype('int')\n",
    "X_wv_train = X_wv_train.astype('int')\n",
    "y_wv_test = y_wv_test.astype('int')\n",
    "X_wv_test = X_wv_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43102714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model. Almost instant\n",
    "lc_bow = LogisticRegression(max_iter=500)\n",
    "lc_tfidf = LogisticRegression(max_iter=500)\n",
    "lc_wv = LogisticRegression(max_iter=500)\n",
    "\n",
    "lc_bow.fit(X_bow_train, y_bow_train)\n",
    "lc_tfidf.fit(X_tfidf_train, y_tfidf_train)\n",
    "lc_wv.fit(X_wv_train, y_wv_train)\n",
    "\n",
    "y_lc_bow_predicted = lc_bow.predict(X_bow_test).astype('int')\n",
    "y_lc_bow_pred_proba = lc_bow.predict_proba(X_bow_test).astype('int')\n",
    "y_lc_tfidf_predicted = lc_tfidf.predict(X_tfidf_test).astype('int')\n",
    "y_lc_tfidf_pred_proba = lc_tfidf.predict_proba(X_tfidf_test).astype('int')\n",
    "y_lc_wv_predicted = lc_wv.predict(X_wv_test).astype('int')\n",
    "y_lc_wv_pred_proba = lc_wv.predict_proba(X_wv_test).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Bag of Words Log Regression', 'TF*IDF Log Regression', 'Word2Vec Log Regression']\n",
    "predictions = [y_lc_bow_predicted, y_lc_tfidf_predicted, y_lc_wv_predicted]\n",
    "pred_probabilities = [y_lc_bow_pred_proba, y_lc_tfidf_pred_proba, y_lc_wv_pred_proba]\n",
    "\n",
    "plot = 1\n",
    "\n",
    "for model, prediction, pred_proba in zip(models, predictions, pred_probabilities):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix(y_bow_test.ravel(), prediction))\n",
    "    disp.plot(\n",
    "        include_values=True,\n",
    "        cmap='gray',\n",
    "        colorbar=False\n",
    "    )\n",
    "    disp.ax_.set_title(f\"{model} Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_bow_test, y_lc_bow_predicted))\n",
    "print(classification_report(y_tfidf_test, y_lc_tfidf_predicted))\n",
    "print(classification_report(y_wv_test, y_lc_wv_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classifier. Takes \n",
    "svc = SVC(probability=True)\n",
    "\n",
    "svc.fit(X_bow_train, y_bow_train)\n",
    "\n",
    "y_svc_predicted = svc.predict(X_bow_test)\n",
    "y_svc_pred_proba = svc.predict_proba(X_bow_test)\n",
    "\n",
    "print(classification_report(y_bow_test, y_svc_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = GaussianNB()\n",
    "\n",
    "nbc.fit(X_bow_train, y_bow_train)\n",
    "\n",
    "y_nbc_predicted = nbc.predict(X_bow_test)\n",
    "y_nbc_pred_proba = nbc.predict_proba(X_bow_test)\n",
    "\n",
    "print(classification_report(y_bow_test, y_nbc_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efe669",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc.fit(X_bow_train, y_bow_train)\n",
    "\n",
    "y_rfc_predicted = rfc.predict(X_bow_test)\n",
    "y_rfc_pred_proba = rfc.predict_proba(X_bow_test)\n",
    "\n",
    "print(classification_report(y_bow_test, y_rfc_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045cea7",
   "metadata": {},
   "source": [
    "<h1>Step 5: Model evaluation.</h1></br> Evaluate your model performance with the\n",
    "provided testing dataset. Recall the evaluation metrics we covered in the class\n",
    "and select appropriate metrics for this problem. Please compare the\n",
    "performance of different classifiers using the same linguistic feature and the\n",
    "performance of the same classifier using different linguistic features. Finally,\n",
    "discuss your experimental results and submit the assignment report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f69764",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Logistic Regression','Support Vector Machine', 'Naive Bayes Classifier', 'Random Forest Classifier']\n",
    "predictions = [y_lc_bow_predicted, y_svc_predicted, y_nbc_predicted, y_rfc_predicted]\n",
    "pred_probabilities = [y_lc_bow_pred_proba, y_svc_pred_proba, y_nbc_pred_proba, y_rfc_pred_proba]\n",
    "\n",
    "plot = 1\n",
    "\n",
    "for model, prediction, pred_proba in zip(models, predictions, pred_probabilities):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix(y_test.ravel(), prediction))\n",
    "    disp.plot(\n",
    "        include_values=True,\n",
    "        cmap='gray',\n",
    "        colorbar=False\n",
    "    )\n",
    "    disp.ax_.set_title(f\"{model} Confusion Matrix\")\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.suptitle(\"ROC Curves\")\n",
    "plot_index = 1\n",
    "\n",
    "for model, prediction, pred_proba in zip(models, predictions, pred_probabilities):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_proba[:, 1])\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    plt.subplot(3, 2, plot_index)\n",
    "    plt.plot(fpr, tpr, 'r', label='ROC curve')\n",
    "    # pyplot.figure(figsize=(5, 5))\n",
    "    plt.title(f'Roc Curve - {model} - [AUC - {auc_score}]', fontsize=14)\n",
    "    plt.xlabel('FPR', fontsize=12)\n",
    "    plt.ylabel('TPR', fontsize=12)\n",
    "    plt.legend()\n",
    "    plot_index += 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
